
@article{kay_kinetics_2017,
	title = {The {Kinetics} {Human} {Action} {Video} {Dataset}},
	url = {http://arxiv.org/abs/1705.06950},
	abstract = {We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.},
	urldate = {2018-11-02},
	journal = {arXiv:1705.06950 [cs]},
	author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
	month = may,
	year = {2017},
	note = {arXiv: 1705.06950},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1705.06950 PDF:/Users/miguel/Zotero/storage/RA4UID7K/Kay et al. - 2017 - The Kinetics Human Action Video Dataset.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/3FH8AJSC/1705.html:text/html}
}

@misc{weng_gan_nodate,
	title = {From {GAN} to {WGAN}},
	url = {https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html#kullbackleibler-and-jensenshannon-divergence},
	urldate = {2018-11-02},
	author = {Weng, Lilian},
	file = {From GAN to WGAN:/Users/miguel/Zotero/storage/YFSPE5SM/from-GAN-to-WGAN.html:text/html}
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	shorttitle = {{NIPS} 2016 {Tutorial}},
	url = {http://arxiv.org/abs/1701.00160},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	language = {en},
	urldate = {2018-11-02},
	journal = {arXiv:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	month = dec,
	year = {2016},
	note = {arXiv: 1701.00160},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: v2-v4 are all typo fixes. No substantive changes relative to v1},
	file = {Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:/Users/miguel/Zotero/storage/X2AMXNDR/Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:application/pdf}
}

@misc{yann_mnist_nodate,
	title = {{MNIST} handwritten digit database, {Yann} {LeCun}, {Corinna} {Cortes} and {Chris} {Burges}},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2018-11-02},
	author = {Yann, LeCun and Corinna, Cortes and Chris, Burges},
	file = {MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges:/Users/miguel/Zotero/storage/7CU55PBL/mnist.html:text/html}
}

@misc{noauthor_microsoft_nodate,
	title = {Microsoft {Research} {Video} {Description} {Corpus}},
	url = {https://www.microsoft.com/en-us/download/details.aspx?id=52422},
	abstract = {This data consists of about 120K sentences collected during the summer of 2010. Last published: November 12, 2010.},
	language = {en-us},
	urldate = {2018-11-02},
	journal = {Microsoft Download Center},
	file = {Snapshot:/Users/miguel/Zotero/storage/EWCE92CP/details.html:text/html}
}

@article{radford_unsupervised_2015,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2018-11-02},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {arXiv\:1511.06434 PDF:/Users/miguel/Zotero/storage/KQUX92QD/Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/MWLSU65R/1511.html:text/html}
}

@incollection{dai_semi-supervised_2015,
	title = {Semi-supervised {Sequence} {Learning}},
	url = {http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf},
	urldate = {2018-10-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Dai, Andrew M and Le, Quoc V},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {3079--3087},
	file = {NIPS Full Text PDF:/Users/miguel/Zotero/storage/5K48GEIQ/Dai and Le - 2015 - Semi-supervised Sequence Learning.pdf:application/pdf;NIPS Snapshot:/Users/miguel/Zotero/storage/73IAB5JD/5949-semi-supervised-sequence-learning.html:text/html}
}

@article{pan_create_2018,
	title = {To {Create} {What} {You} {Tell}: {Generating} {Videos} from {Captions}},
	shorttitle = {To {Create} {What} {You} {Tell}},
	url = {http://arxiv.org/abs/1804.08264},
	abstract = {We are creating multimedia contents everyday and everywhere. While automatic content generation has played a fundamental challenge to multimedia community for decades, recent advances of deep learning have made this problem feasible. For example, the Generative Adversarial Networks (GANs) is a rewarding approach to synthesize images. Nevertheless, it is not trivial when capitalizing on GANs to generate videos. The difficulty originates from the intrinsic structure where a video is a sequence of visually coherent and semantically dependent frames. This motivates us to explore semantic and temporal coherence in designing GANs to generate videos. In this paper, we present a novel Temporal GANs conditioning on Captions, namely TGANs-C, in which the input to the generator network is a concatenation of a latent noise vector and caption embedding, and then is transformed into a frame sequence with 3D spatio-temporal convolutions. Unlike the naive discriminator which only judges pairs as fake or real, our discriminator additionally notes whether the video matches the correct caption. In particular, the discriminator network consists of three discriminators: video discriminator classifying realistic videos from generated ones and optimizes video-caption matching, frame discriminator discriminating between real and fake frames and aligning frames with the conditioning caption, and motion discriminator emphasizing the philosophy that the adjacent frames in the generated videos should be smoothly connected as in real ones. We qualitatively demonstrate the capability of our TGANs-C to generate plausible videos conditioning on the given captions on two synthetic datasets (SBMG and TBMG) and one real-world dataset (MSVD). Moreover, quantitative experiments on MSVD are performed to validate our proposal via Generative Adversarial Metric and human study.},
	urldate = {2018-10-19},
	journal = {arXiv:1804.08264 [cs]},
	author = {Pan, Yingwei and Qiu, Zhaofan and Yao, Ting and Li, Houqiang and Mei, Tao},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.08264},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ACM MM 2017 Brave New Idea},
	file = {arXiv\:1804.08264 PDF:/Users/miguel/Zotero/storage/7UI9DU7W/Pan et al. - 2018 - To Create What You Tell Generating Videos from Ca.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/Z87W24XF/1804.html:text/html}
}

@article{kingma_glow:_2018,
	title = {Glow: {Generative} {Flow} with {Invertible} 1x1 {Convolutions}},
	shorttitle = {Glow},
	url = {http://arxiv.org/abs/1807.03039},
	abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
	urldate = {2018-08-22},
	journal = {arXiv:1807.03039 [cs, stat]},
	author = {Kingma, Diederik P. and Dhariwal, Prafulla},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.03039},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 15 pages; fixed typo in abstract},
	file = {arXiv\:1807.03039 PDF:/Users/miguel/Zotero/storage/NYXW2JBW/Kingma and Dhariwal - 2018 - Glow Generative Flow with Invertible 1x1 Convolut.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/AYKARQMN/1807.html:text/html}
}

@article{gulrajani_improved_2017,
	title = {Improved {Training} of {Wasserstein} {GANs}},
	url = {http://arxiv.org/abs/1704.00028},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
	urldate = {2018-04-10},
	journal = {arXiv:1704.00028 [cs, stat]},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	month = mar,
	year = {2017},
	note = {arXiv: 1704.00028},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS camera-ready},
	file = {arXiv\:1704.00028 PDF:/Users/miguel/Zotero/storage/662D9SL3/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/SZ724JFS/1704.html:text/html}
}

@article{zhang_stackgan++:_2017,
	title = {{StackGAN}++: {Realistic} {Image} {Synthesis} with {Stacked} {Generative} {Adversarial} {Networks}},
	shorttitle = {{StackGAN}++},
	url = {http://arxiv.org/abs/1710.10916},
	abstract = {Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) aiming at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of the object based on given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and discriminators in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.},
	urldate = {2018-04-10},
	journal = {arXiv:1710.10916 [cs, stat]},
	author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.10916},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 14 pages, 14 figures. arXiv admin note: text overlap with arXiv:1612.03242},
	file = {arXiv\:1710.10916 PDF:/Users/miguel/Zotero/storage/GAEUYWN6/Zhang et al. - 2017 - StackGAN++ Realistic Image Synthesis with Stacked.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/W9TH4YEU/1710.html:text/html}
}

@article{li_video_2017,
	title = {Video {Generation} {From} {Text}},
	url = {http://arxiv.org/abs/1710.00421},
	abstract = {Generating videos from text has proven to be a significant challenge for existing generative models. We tackle this problem by training a conditional generative model to extract both static and dynamic information from text. This is manifested in a hybrid framework, employing a Variational Autoencoder (VAE) and a Generative Adversarial Network (GAN). The static features, called "gist," are used to sketch text-conditioned background color and object layout structure. Dynamic features are considered by transforming input text into an image filter. To obtain a large amount of data for training the deep-learning model, we develop a method to automatically create a matched text-video corpus from publicly available online videos. Experimental results show that the proposed framework generates plausible and diverse videos, while accurately reflecting the input text information. It significantly outperforms baseline models that directly adapt text-to-image generation procedures to produce videos. Performance is evaluated both visually and by adapting the inception score used to evaluate image generation in GANs.},
	urldate = {2018-04-10},
	journal = {arXiv:1710.00421 [cs]},
	author = {Li, Yitong and Min, Martin Renqiang and Shen, Dinghan and Carlson, David and Carin, Lawrence},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.00421},
	keywords = {Computer Science - Multimedia},
	file = {arXiv\:1710.00421 PDF:/Users/miguel/Zotero/storage/ZNGF2S5R/Li et al. - 2017 - Video Generation From Text.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/MRTIPKK8/1710.html:text/html}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2018-01-16},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1502.03167 PDF:/Users/miguel/Zotero/storage/X7TGHC8A/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/2JEW2MB3/1502.html:text/html}
}

@article{brock_large_2018,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1809.11096},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
	urldate = {2019-03-23},
	journal = {arXiv:1809.11096 [cs, stat]},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.11096},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1809.11096 PDF:/Users/miguel/Zotero/storage/6XFHAE84/Brock et al. - 2018 - Large Scale GAN Training for High Fidelity Natural.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/QHMZMLQW/1809.html:text/html}
}

@article{wang_non-local_2017,
	title = {Non-local {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.07971},
	abstract = {Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code is available at https://github.com/facebookresearch/video-nonlocal-net .},
	urldate = {2019-06-07},
	journal = {arXiv:1711.07971 [cs]},
	author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.07971},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2018, code is available at: https://github.com/facebookresearch/video-nonlocal-net},
	file = {arXiv\:1711.07971 PDF:/Users/miguel/Zotero/storage/GB8IPAIX/Wang et al. - 2017 - Non-local Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/SPJJBSAD/1711.html:text/html}
}

@article{jolicoeur-martineau_relativistic_2018,
	title = {The relativistic discriminator: a key element missing from standard {GAN}},
	shorttitle = {The relativistic discriminator},
	url = {http://arxiv.org/abs/1807.00734},
	abstract = {In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400\%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.},
	urldate = {2019-06-07},
	journal = {arXiv:1807.00734 [cs, stat]},
	author = {Jolicoeur-Martineau, Alexia},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.00734},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: https://github.com/AlexiaJM/RelativisticGAN},
	file = {arXiv\:1807.00734 PDF:/Users/miguel/Zotero/storage/T55XWFMX/Jolicoeur-Martineau - 2018 - The relativistic discriminator a key element miss.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/4F6A7SF2/1807.html:text/html}
}

@article{saito_tganv2:_2018,
	title = {{TGANv}2: {Efficient} {Training} of {Large} {Models} for {Video} {Generation} with {Multiple} {Subsampling} {Layers}},
	shorttitle = {{TGANv}2},
	url = {http://arxiv.org/abs/1811.09245},
	abstract = {In this paper, we propose a novel method to efficiently train a Generative Adversarial Network (GAN) on high dimensional samples. The key idea is to introduce a differentiable subsampling layer which appropriately reduces the dimensionality of intermediate feature maps in the generator during training. In general, generators require large memory and computational costs in the latter stages of the network as the feature maps become larger, though the latter stages have relatively fewer parameters than the earlier stages. It makes training large models for video generation difficult due to the limited computational resource. We solve this problem by introducing a method that gradually reduces the dimensionality of feature maps in the generator with multiple subsampling layers. We also propose a network (Temporal GAN v2) with such layers and perform video generation experiments. As a consequence, our model trained on the UCF101 dataset at \$192 {\textbackslash}times 192\$ pixels achieves an Inception Score (IS) of 24.34, which shows a significant improvement over the previous state-of-the-art score of 14.56.},
	urldate = {2019-06-07},
	journal = {arXiv:1811.09245 [cs]},
	author = {Saito, Masaki and Saito, Shunta},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.09245},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: The code will be released soon},
	file = {arXiv\:1811.09245 PDF:/Users/miguel/Zotero/storage/SVJIETRS/Saito and Saito - 2018 - TGANv2 Efficient Training of Large Models for Vid.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/HB5GJ9S9/1811.html:text/html}
}

@article{carreira_quo_2017,
	title = {Quo {Vadis}, {Action} {Recognition}? {A} {New} {Model} and the {Kinetics} {Dataset}},
	shorttitle = {Quo {Vadis}, {Action} {Recognition}?},
	url = {http://arxiv.org/abs/1705.07750},
	abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9\% on HMDB-51 and 98.0\% on UCF-101.},
	urldate = {2019-06-11},
	journal = {arXiv:1705.07750 [cs]},
	author = {Carreira, Joao and Zisserman, Andrew},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07750},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Removed references to mini-kinetics dataset that was never made publicly available and repeated all experiments on the full Kinetics dataset},
	file = {arXiv\:1705.07750 PDF:/Users/miguel/Zotero/storage/ZFTWM9JX/Carreira and Zisserman - 2017 - Quo Vadis, Action Recognition A New Model and the.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/DZ8VDPX7/1705.html:text/html}
}

@article{zhang_self-attention_2018,
	title = {Self-{Attention} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1805.08318},
	abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
	urldate = {2019-06-11},
	journal = {arXiv:1805.08318 [cs, stat]},
	author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08318},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv\:1805.08318 PDF:/Users/miguel/Zotero/storage/I8QAPUHW/Zhang et al. - 2018 - Self-Attention Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/I3Q683QQ/1805.html:text/html}
}

@inproceedings{xu_msr-vtt:_2016,
	address = {Las Vegas, NV, USA},
	title = {{MSR}-{VTT}: {A} {Large} {Video} {Description} {Dataset} for {Bridging} {Video} and {Language}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {{MSR}-{VTT}},
	url = {http://ieeexplore.ieee.org/document/7780940/},
	doi = {10.1109/CVPR.2016.571},
	abstract = {While there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on speciﬁc ﬁne-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content.},
	language = {en},
	urldate = {2019-06-11},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
	month = jun,
	year = {2016},
	pages = {5288--5296},
	file = {Xu et al. - 2016 - MSR-VTT A Large Video Description Dataset for Bri.pdf:/Users/miguel/Zotero/storage/GVVYIKUX/Xu et al. - 2016 - MSR-VTT A Large Video Description Dataset for Bri.pdf:application/pdf}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2019-06-11},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1312.6114 PDF:/Users/miguel/Zotero/storage/VWNRNDGW/Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/A2Q8D47C/1312.html:text/html}
}

@article{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	urldate = {2019-06-13},
	journal = {arXiv:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1784},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1411.1784 PDF:/Users/miguel/Zotero/storage/8PL5EWCA/Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/3EECHPBD/1411.html:text/html}
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2019-06-13},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1406.2661 PDF:/Users/miguel/Zotero/storage/R6Q7TLGK/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/44RZH9VI/1406.html:text/html}
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	annote = {http://www.deeplearningbook.org}
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2019-06-13},
	journal = {arXiv:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.07875},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1701.07875 PDF:/Users/miguel/Zotero/storage/DAFFYQY3/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/323BKEA5/1701.html:text/html}
}

@article{saito_temporal_2016,
	title = {Temporal {Generative} {Adversarial} {Nets} with {Singular} {Value} {Clipping}},
	url = {http://arxiv.org/abs/1611.06624},
	abstract = {In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods.},
	urldate = {2019-06-13},
	journal = {arXiv:1611.06624 [cs]},
	author = {Saito, Masaki and Matsumoto, Eiichi and Saito, Shunta},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.06624},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: to appear in ICCV 2017},
	file = {arXiv\:1611.06624 PDF:/Users/miguel/Zotero/storage/3X4QHG2Q/Saito et al. - 2016 - Temporal Generative Adversarial Nets with Singular.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/7QAQIQDD/1611.html:text/html}
}

@article{heusel_gans_2017,
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	url = {http://arxiv.org/abs/1706.08500},
	abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr{\textbackslash}'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
	urldate = {2019-06-13},
	journal = {arXiv:1706.08500 [cs, stat]},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.08500},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Implementations are available at: https://github.com/bioinf-jku/TTUR},
	file = {arXiv\:1706.08500 PDF:/Users/miguel/Zotero/storage/4DAIEH29/Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/V6D49WB8/1706.html:text/html}
}

@article{salimans_improved_2016,
	title = {Improved {Techniques} for {Training} {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
	urldate = {2019-06-13},
	journal = {arXiv:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03498},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1606.03498 PDF:/Users/miguel/Zotero/storage/7ZBV9D9W/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/SWGV93C7/1606.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2019-06-13},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv\:1512.03385 PDF:/Users/miguel/Zotero/storage/4HLQV9R8/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/G8TLVANI/1512.html:text/html}
}

@article{saito_temporal_2016-1,
	title = {Temporal {Generative} {Adversarial} {Nets} with {Singular} {Value} {Clipping}},
	url = {http://arxiv.org/abs/1611.06624},
	abstract = {In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods.},
	urldate = {2019-06-14},
	journal = {arXiv:1611.06624 [cs]},
	author = {Saito, Masaki and Matsumoto, Eiichi and Saito, Shunta},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.06624},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: to appear in ICCV 2017},
	file = {arXiv\:1611.06624 PDF:/Users/miguel/Zotero/storage/HY24CLG3/Saito et al. - 2016 - Temporal Generative Adversarial Nets with Singular.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/KUUSBLEH/1611.html:text/html}
}

@article{shi_convolutional_2015,
	title = {Convolutional {LSTM} {Network}: {A} {Machine} {Learning} {Approach} for {Precipitation} {Nowcasting}},
	shorttitle = {Convolutional {LSTM} {Network}},
	url = {http://arxiv.org/abs/1506.04214},
	abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
	urldate = {2019-06-14},
	journal = {arXiv:1506.04214 [cs]},
	author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04214},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1506.04214 PDF:/Users/miguel/Zotero/storage/SLD6ABJ8/Shi et al. - 2015 - Convolutional LSTM Network A Machine Learning App.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/PGZ3I72U/1506.html:text/html}
}

@article{jacobsen_i-revnet:_2018,
	title = {i-{RevNet}: {Deep} {Invertible} {Networks}},
	shorttitle = {i-{RevNet}},
	url = {http://arxiv.org/abs/1802.07088},
	abstract = {It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet we reconstruct linear interpolations between natural image representations.},
	urldate = {2019-06-14},
	journal = {arXiv:1802.07088 [cs, stat]},
	author = {Jacobsen, Jörn-Henrik and Smeulders, Arnold and Oyallon, Edouard},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.07088},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1802.07088 PDF:/Users/miguel/Zotero/storage/BHPQQYSE/Jacobsen et al. - 2018 - i-RevNet Deep Invertible Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/miguel/Zotero/storage/EF5CTCML/1802.html:text/html}
}

@article{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	month = feb,
	year = {2019},
	pages = {24},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/Users/miguel/Zotero/storage/T9M56TKB/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf}
}

@misc{martin_miguelmartin75/txt2vid_nodate,
	title = {miguelmartin75/txt2vid},
	url = {https://github.com/miguelmartin75/txt2vid},
	abstract = {Source code for "Learning to Generate Videos from Text" honours project},
	language = {en},
	urldate = {2019-06-14},
	journal = {GitHub},
	author = {Martin, Miguel},
	file = {Snapshot:/Users/miguel/Zotero/storage/37DC8RC9/txt2vid.html:text/html}
}

@misc{noauthor_ucf101_nodate,
	title = {{UCF}101 - {Action} {Recognition} {Data} {Set}},
	url = {https://www.crcv.ucf.edu/data/UCF101.php},
	urldate = {2019-06-14},
	file = {CRCV | Center for Research in Computer Vision at the University of Central Florida:/Users/miguel/Zotero/storage/ZZ584SK2/UCF101.html:text/html}
}