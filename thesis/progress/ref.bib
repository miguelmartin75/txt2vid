
@article{gupta_imagine_2018,
	title = {Imagine {This}! {Scripts} to {Compositions} to {Videos}},
	url = {http://arxiv.org/abs/1804.03608},
	abstract = {Imagining a scene described in natural language with realistic layout and appearance of entities is the ultimate test of spatial, visual, and semantic world knowledge. Towards this goal, we present the Composition, Retrieval, and Fusion Network (CRAFT), a model capable of learning this knowledge from video-caption data and applying it while generating videos from novel captions. CRAFT explicitly predicts a temporal-layout of mentioned entities (characters and objects), retrieves spatio-temporal entity segments from a video database and fuses them to generate scene videos. Our contributions include sequential training of components of CRAFT while jointly modeling layout and appearances, and losses that encourage learning compositional representations for retrieval. We evaluate CRAFT on semantic fidelity to caption, composition consistency, and visual quality. CRAFT outperforms direct pixel generation approaches and generalizes well to unseen captions and to unseen video databases with no text annotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated video-caption dataset with over 25000 videos. For a glimpse of videos generated by CRAFT, see https://youtu.be/688Vv86n0z8.},
	urldate = {2018-07-19},
	journal = {arXiv:1804.03608 [cs]},
	author = {Gupta, Tanmay and Schwenk, Dustin and Farhadi, Ali and Hoiem, Derek and Kembhavi, Aniruddha},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.03608},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	annote = {Comment: Supplementary material included},
	file = {arXiv\:1804.03608 PDF:/home/doubleu/Zotero/storage/66I3262J/Gupta et al. - 2018 - Imagine This! Scripts to Compositions to Videos.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/XNRCG7DL/1804.html:text/html}
}

@article{gulrajani_improved_2017,
	title = {Improved {Training} of {Wasserstein} {GANs}},
	url = {http://arxiv.org/abs/1704.00028},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
	urldate = {2018-04-10},
	journal = {arXiv:1704.00028 [cs, stat]},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	month = mar,
	year = {2017},
	note = {arXiv: 1704.00028},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS camera-ready},
	file = {arXiv\:1704.00028 PDF:/home/doubleu/Zotero/storage/662D9SL3/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/SZ724JFS/1704.html:text/html}
}

@article{zhang_stackgan++:_2017,
	title = {{StackGAN}++: {Realistic} {Image} {Synthesis} with {Stacked} {Generative} {Adversarial} {Networks}},
	shorttitle = {{StackGAN}++},
	url = {http://arxiv.org/abs/1710.10916},
	abstract = {Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) aiming at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of the object based on given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and discriminators in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.},
	urldate = {2018-04-10},
	journal = {arXiv:1710.10916 [cs, stat]},
	author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.10916},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	annote = {Comment: 14 pages, 14 figures. arXiv admin note: text overlap with arXiv:1612.03242},
	file = {arXiv\:1710.10916 PDF:/home/doubleu/Zotero/storage/GAEUYWN6/Zhang et al. - 2017 - StackGAN++ Realistic Image Synthesis with Stacked.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/W9TH4YEU/1710.html:text/html}
}

@article{karras_progressive_2017,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	urldate = {2018-04-10},
	journal = {arXiv:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.10196},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Final ICLR 2018 version},
	file = {arXiv\:1710.10196 PDF:/home/doubleu/Zotero/storage/NSTFG4KD/Karras et al. - 2017 - Progressive Growing of GANs for Improved Quality, .pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/M8VPHDUG/1710.html:text/html}
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2018-04-10},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1406.2661 PDF:/home/doubleu/Zotero/storage/KIX7HLQF/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/MXWM57RI/1406.html:text/html}
}

@article{li_video_2017,
	title = {Video {Generation} {From} {Text}},
	url = {http://arxiv.org/abs/1710.00421},
	abstract = {Generating videos from text has proven to be a significant challenge for existing generative models. We tackle this problem by training a conditional generative model to extract both static and dynamic information from text. This is manifested in a hybrid framework, employing a Variational Autoencoder (VAE) and a Generative Adversarial Network (GAN). The static features, called "gist," are used to sketch text-conditioned background color and object layout structure. Dynamic features are considered by transforming input text into an image filter. To obtain a large amount of data for training the deep-learning model, we develop a method to automatically create a matched text-video corpus from publicly available online videos. Experimental results show that the proposed framework generates plausible and diverse videos, while accurately reflecting the input text information. It significantly outperforms baseline models that directly adapt text-to-image generation procedures to produce videos. Performance is evaluated both visually and by adapting the inception score used to evaluate image generation in GANs.},
	urldate = {2018-04-10},
	journal = {arXiv:1710.00421 [cs]},
	author = {Li, Yitong and Min, Martin Renqiang and Shen, Dinghan and Carlson, David and Carin, Lawrence},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.00421},
	keywords = {Computer Science - Multimedia},
	file = {arXiv\:1710.00421 PDF:/home/doubleu/Zotero/storage/ZNGF2S5R/Li et al. - 2017 - Video Generation From Text.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/MRTIPKK8/1710.html:text/html}
}

@article{johnson_image_2018,
	title = {Image {Generation} from {Scene} {Graphs}},
	url = {http://arxiv.org/abs/1804.01622},
	abstract = {To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.},
	urldate = {2018-08-08},
	journal = {arXiv:1804.01622 [cs]},
	author = {Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.01622},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: To appear at CVPR 2018},
	file = {arXiv\:1804.01622 PDF:/home/doubleu/Zotero/storage/Z576HUJT/Johnson et al. - 2018 - Image Generation from Scene Graphs.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/EBJJLJC3/1804.html:text/html}
}

@article{kingma_glow:_2018,
	title = {Glow: {Generative} {Flow} with {Invertible} 1x1 {Convolutions}},
	shorttitle = {Glow},
	url = {http://arxiv.org/abs/1807.03039},
	abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
	urldate = {2018-08-22},
	journal = {arXiv:1807.03039 [cs, stat]},
	author = {Kingma, Diederik P. and Dhariwal, Prafulla},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.03039},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 15 pages; fixed typo in abstract},
	file = {arXiv\:1807.03039 PDF:/home/doubleu/Zotero/storage/NYXW2JBW/Kingma and Dhariwal - 2018 - Glow Generative Flow with Invertible 1x1 Convolut.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/AYKARQMN/1807.html:text/html}
}

@inproceedings{zhang_stackgan:_2017,
	address = {Venice},
	title = {{StackGAN}: {Text} to {Photo}-{Realistic} {Image} {Synthesis} with {Stacked} {Generative} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {{StackGAN}},
	url = {http://ieeexplore.ieee.org/document/8237891/},
	doi = {10.1109/ICCV.2017.629},
	abstract = {Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto-image approaches can roughly reﬂect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256⇥256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-reﬁnement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the reﬁnement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves signiﬁcant improvements on generating photo-realistic images conditioned on text descriptions.},
	language = {en},
	urldate = {2018-08-22},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhang, Han and Xu, Tao and Li, Hongsheng},
	month = oct,
	year = {2017},
	pages = {5908--5916},
	file = {Zhang et al. - 2017 - StackGAN Text to Photo-Realistic Image Synthesis .pdf:/home/doubleu/Zotero/storage/JR6SW78W/Zhang et al. - 2017 - StackGAN Text to Photo-Realistic Image Synthesis .pdf:application/pdf}
}

@article{pan_create_2018,
	title = {To {Create} {What} {You} {Tell}: {Generating} {Videos} from {Captions}},
	shorttitle = {To {Create} {What} {You} {Tell}},
	url = {http://arxiv.org/abs/1804.08264},
	abstract = {We are creating multimedia contents everyday and everywhere. While automatic content generation has played a fundamental challenge to multimedia community for decades, recent advances of deep learning have made this problem feasible. For example, the Generative Adversarial Networks (GANs) is a rewarding approach to synthesize images. Nevertheless, it is not trivial when capitalizing on GANs to generate videos. The difficulty originates from the intrinsic structure where a video is a sequence of visually coherent and semantically dependent frames. This motivates us to explore semantic and temporal coherence in designing GANs to generate videos. In this paper, we present a novel Temporal GANs conditioning on Captions, namely TGANs-C, in which the input to the generator network is a concatenation of a latent noise vector and caption embedding, and then is transformed into a frame sequence with 3D spatio-temporal convolutions. Unlike the naive discriminator which only judges pairs as fake or real, our discriminator additionally notes whether the video matches the correct caption. In particular, the discriminator network consists of three discriminators: video discriminator classifying realistic videos from generated ones and optimizes video-caption matching, frame discriminator discriminating between real and fake frames and aligning frames with the conditioning caption, and motion discriminator emphasizing the philosophy that the adjacent frames in the generated videos should be smoothly connected as in real ones. We qualitatively demonstrate the capability of our TGANs-C to generate plausible videos conditioning on the given captions on two synthetic datasets (SBMG and TBMG) and one real-world dataset (MSVD). Moreover, quantitative experiments on MSVD are performed to validate our proposal via Generative Adversarial Metric and human study.},
	urldate = {2018-10-19},
	journal = {arXiv:1804.08264 [cs]},
	author = {Pan, Yingwei and Qiu, Zhaofan and Yao, Ting and Li, Houqiang and Mei, Tao},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.08264},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ACM MM 2017 Brave New Idea},
	file = {arXiv\:1804.08264 PDF:/home/doubleu/Zotero/storage/7UI9DU7W/Pan et al. - 2018 - To Create What You Tell Generating Videos from Ca.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/Z87W24XF/1804.html:text/html}
}

@incollection{dai_semi-supervised_2015,
	title = {Semi-supervised {Sequence} {Learning}},
	url = {http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf},
	urldate = {2018-10-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Dai, Andrew M and Le, Quoc V},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {3079--3087},
	file = {NIPS Full Text PDF:/home/doubleu/Zotero/storage/5K48GEIQ/Dai and Le - 2015 - Semi-supervised Sequence Learning.pdf:application/pdf;NIPS Snapshot:/home/doubleu/Zotero/storage/73IAB5JD/5949-semi-supervised-sequence-learning.html:text/html}
}

@article{vondrick_generating_nodate,
	title = {Generating {Videos} with {Scene} {Dynamics}},
	abstract = {We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classiﬁcation) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene’s foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.},
	language = {en},
	author = {Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
	pages = {10},
	file = {Vondrick et al. - Generating Videos with Scene Dynamics.pdf:/home/doubleu/Zotero/storage/EDBVBDIK/Vondrick et al. - Generating Videos with Scene Dynamics.pdf:application/pdf}
}

@article{radford_unsupervised_2015,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2018-11-02},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {arXiv\:1511.06434 PDF:/home/doubleu/Zotero/storage/KQUX92QD/Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/MWLSU65R/1511.html:text/html}
}

@misc{noauthor_microsoft_nodate,
	title = {Microsoft {Research} {Video} {Description} {Corpus}},
	url = {https://www.microsoft.com/en-us/download/details.aspx?id=52422},
	abstract = {This data consists of about 120K sentences collected during the summer of 2010. Last published: November 12, 2010.},
	language = {en-us},
	urldate = {2018-11-02},
	journal = {Microsoft Download Center},
	file = {Snapshot:/home/doubleu/Zotero/storage/EWCE92CP/details.html:text/html}
}

@misc{yann_mnist_nodate,
	title = {{MNIST} handwritten digit database, {Yann} {LeCun}, {Corinna} {Cortes} and {Chris} {Burges}},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2018-11-02},
	author = {Yann, LeCun and Corinna, Cortes and Chris, Burges},
	file = {MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges:/home/doubleu/Zotero/storage/7CU55PBL/mnist.html:text/html}
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	url = {https://openreview.net/forum?id=BJJsrmfCZ},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua...},
	urldate = {2018-11-02},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	month = oct,
	year = {2017},
	file = {Full Text PDF:/home/doubleu/Zotero/storage/DDK3934E/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf:application/pdf;Snapshot:/home/doubleu/Zotero/storage/NDD3TF2F/forum.html:text/html}
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	shorttitle = {{NIPS} 2016 {Tutorial}},
	url = {http://arxiv.org/abs/1701.00160},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	language = {en},
	urldate = {2018-11-02},
	journal = {arXiv:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	month = dec,
	year = {2016},
	note = {arXiv: 1701.00160},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: v2-v4 are all typo fixes. No substantive changes relative to v1},
	file = {Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:/home/doubleu/Zotero/storage/X2AMXNDR/Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:application/pdf}
}

@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2018-11-02},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv\:1412.6980 PDF:/home/doubleu/Zotero/storage/ALLACZV5/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/763E3WSG/1412.html:text/html}
}

@article{springenberg_striving_2014,
	title = {Striving for {Simplicity}: {The} {All} {Convolutional} {Net}},
	shorttitle = {Striving for {Simplicity}},
	url = {http://arxiv.org/abs/1412.6806},
	abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
	urldate = {2018-11-02},
	journal = {arXiv:1412.6806 [cs]},
	author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6806},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: accepted to ICLR-2015 workshop track; no changes other than style},
	file = {arXiv\:1412.6806 PDF:/home/doubleu/Zotero/storage/9P3276IQ/Springenberg et al. - 2014 - Striving for Simplicity The All Convolutional Net.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/5TSQESQY/1412.html:text/html}
}

@misc{weng_gan_nodate,
	title = {From {GAN} to {WGAN}},
	url = {https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html#kullbackleibler-and-jensenshannon-divergence},
	urldate = {2018-11-02},
	author = {Weng, Lilian},
	file = {From GAN to WGAN:/home/doubleu/Zotero/storage/YFSPE5SM/from-GAN-to-WGAN.html:text/html}
}

@article{xu_empirical_2015,
	title = {Empirical {Evaluation} of {Rectified} {Activations} in {Convolutional} {Network}},
	url = {https://arxiv.org/abs/1505.00853},
	language = {en},
	urldate = {2018-11-02},
	author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
	month = may,
	year = {2015},
	file = {Full Text PDF:/home/doubleu/Zotero/storage/ZGWQPYYX/Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in C.pdf:application/pdf;Snapshot:/home/doubleu/Zotero/storage/2EMW7TS6/1505.html:text/html}
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {https://arxiv.org/abs/1701.07875},
	language = {en},
	urldate = {2018-11-02},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jan,
	year = {2017},
	file = {Full Text PDF:/home/doubleu/Zotero/storage/HZZFV86W/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;Snapshot:/home/doubleu/Zotero/storage/92SP3DPL/1701.html:text/html}
}

@article{xu_attngan:_2017,
	title = {{AttnGAN}: {Fine}-{Grained} {Text} to {Image} {Generation} with {Attentional} {Generative} {Adversarial} {Networks}},
	shorttitle = {{AttnGAN}},
	url = {http://arxiv.org/abs/1711.10485},
	abstract = {In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14\% on the CUB dataset and 170.25\% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.},
	urldate = {2018-11-02},
	journal = {arXiv:1711.10485 [cs]},
	author = {Xu, Tao and Zhang, Pengchuan and Huang, Qiuyuan and Zhang, Han and Gan, Zhe and Huang, Xiaolei and He, Xiaodong},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10485},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1711.10485 PDF:/home/doubleu/Zotero/storage/RRVXD396/Xu et al. - 2017 - AttnGAN Fine-Grained Text to Image Generation wit.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/M92JZKZZ/1711.html:text/html}
}

@article{mackay_reversible_2018,
	title = {Reversible {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1810.10999},
	abstract = {Recurrent neural networks (RNNs) provide state-of-the-art performance in processing sequential data but are memory intensive to train, limiting the flexibility of RNN models which can be trained. Reversible RNNs---RNNs for which the hidden-to-hidden transition can be reversed---offer a path to reduce the memory requirements of training, as hidden states need not be stored and instead can be recomputed during backpropagation. We first show that perfectly reversible RNNs, which require no storage of the hidden activations, are fundamentally limited because they cannot forget information from their hidden state. We then provide a scheme for storing a small number of bits in order to allow perfect reversal with forgetting. Our method achieves comparable performance to traditional models while reducing the activation memory cost by a factor of 10--15. We extend our technique to attention-based sequence-to-sequence models, where it maintains performance while reducing activation memory cost by a factor of 5--10 in the encoder, and a factor of 10--15 in the decoder.},
	urldate = {2018-11-02},
	journal = {arXiv:1810.10999 [cs, stat]},
	author = {MacKay, Matthew and Vicol, Paul and Ba, Jimmy and Grosse, Roger},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.10999},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at NIPS 2018},
	file = {arXiv\:1810.10999 PDF:/home/doubleu/Zotero/storage/76RJJVGZ/MacKay et al. - 2018 - Reversible Recurrent Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/JM73EBPM/1810.html:text/html}
}

@article{kay_kinetics_2017,
	title = {The {Kinetics} {Human} {Action} {Video} {Dataset}},
	url = {http://arxiv.org/abs/1705.06950},
	abstract = {We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.},
	urldate = {2018-11-02},
	journal = {arXiv:1705.06950 [cs]},
	author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
	month = may,
	year = {2017},
	note = {arXiv: 1705.06950},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1705.06950 PDF:/home/doubleu/Zotero/storage/RA4UID7K/Kay et al. - 2017 - The Kinetics Human Action Video Dataset.pdf:application/pdf;arXiv.org Snapshot:/home/doubleu/Zotero/storage/3FH8AJSC/1705.html:text/html}
}