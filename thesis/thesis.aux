\relax 
\citation{martin_miguelmartin75/txt2vid_nodate}
\citation{noauthor_microsoft_nodate}
\citation{noauthor_microsoft_nodate}
\@writefile{toc}{\contentsline {chapter}{Introduction}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Project Aims and Contributions}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.2}The Problem At Hand}{2}\protected@file@percent }
\newlabel{fig:example}{{0.2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.2.1}{\ignorespaces `a cat eats corn-on-the-cob' (top), `a woman cuts some lettuce' (bottom). Raw data MRVDC \cite  {noauthor_microsoft_nodate}}}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Attributes of a Solution}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Thesis Outline}{3}\protected@file@percent }
\citation{goodfellow_deep_2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Background}{4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Machine Learning}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Supervised Learning}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Unsupervised Learning}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Auto-Encoders}{4}\protected@file@percent }
\citation{goodfellow_generative_2014}
\citation{goodfellow_generative_2014}
\citation{radford_unsupervised_2015}
\citation{gulrajani_improved_2017}
\citation{goodfellow_generative_2014}
\citation{gulrajani_improved_2017}
\citation{arjovsky_wasserstein_2017}
\citation{jolicoeur-martineau_relativistic_2018}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Generative Modelling}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}GANs}{5}\protected@file@percent }
\newlabel{eq:gan}{{1.6.1}{5}}
\newlabel{ganalgo}{{1.6}{5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces A generalisation of the GAN algorithm. $n_D$ is the number of discriminator steps ($n_D = 1$), $n_G$ is the number of generator steps ($n_G = 1$), $\theta $ are the parameters for the generator and $\omega $ are the parameters for the discriminator}}{5}\protected@file@percent }
\citation{mirza_conditional_2014}
\citation{pan_create_2018}
\citation{zhang_stackgan++:_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Conditional GANs}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Transfer Learning}{5}\protected@file@percent }
\citation{heusel_gans_2017}
\citation{salimans_improved_2016}
\citation{carreira_quo_2017}
\citation{carreira_quo_2017}
\citation{carreira_quo_2017}
\citation{carreira_quo_2017}
\citation{carreira_quo_2017}
\citation{wang_non-local_2017}
\citation{kay_kinetics_2017}
\citation{wang_non-local_2017}
\citation{carreira_quo_2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Video Classification}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Inflated Convolutions}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Two-Stream Network (I3D)}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Non-Local Blocks}{6}\protected@file@percent }
\citation{he_deep_2015}
\citation{wang_non-local_2017}
\citation{he_deep_2015}
\citation{wang_non-local_2017}
\citation{wang_non-local_2017}
\citation{zhang_stackgan++:_2017}
\citation{zhang_stackgan++:_2017}
\citation{brock_large_2018}
\citation{zhang_self-attention_2018}
\citation{wang_non-local_2017}
\newlabel{eq:non_local}{{2.1.1}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Generating Images}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}StackGAN++}{7}\protected@file@percent }
\citation{brock_large_2018}
\citation{li_video_2017}
\citation{radford_unsupervised_2015}
\citation{li_video_2017}
\citation{arjovsky_wasserstein_2017}
\citation{li_video_2017}
\citation{kay_kinetics_2017}
\citation{pan_create_2018}
\citation{li_video_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}BigGAN}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Generating Videos}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Video Generation From Text}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}To Create What You Tell: Generating Videos from Captions}{8}\protected@file@percent }
\citation{noauthor_microsoft_nodate}
\citation{saito_temporal_2016}
\citation{saito_tganv2:_2018}
\citation{shi_convolutional_2015}
\citation{zhang_stackgan++:_2017}
\citation{jolicoeur-martineau_relativistic_2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}TGAN and TGANv2}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Relativistic Discriminator}{9}\protected@file@percent }
\newlabel{eq:rel_loss_discrim}{{2.4.1}{9}}
\newlabel{eq:rel_loss_gen}{{2.4.2}{9}}
\citation{zhang_stackgan++:_2017}
\citation{brock_large_2018}
\citation{pan_create_2018}
\citation{kingma_glow:_2018}
\citation{gulrajani_improved_2017}
\citation{jacobsen_i-revnet:_2018}
\citation{kingma_glow:_2018}
\citation{kingma_glow:_2018}
\citation{pan_create_2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Proposed Solution}{10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Why GANs?}{10}\protected@file@percent }
\citation{saito_tganv2:_2018}
\citation{pan_create_2018}
\citation{pan_create_2018}
\citation{dai_semi-supervised_2015}
\citation{radford_language_2019}
\citation{brock_large_2018}
\citation{saito_tganv2:_2018}
\citation{saito_tganv2:_2018}
\citation{zhang_self-attention_2018}
\citation{brock_large_2018}
\citation{zhang_stackgan++:_2017}
\citation{pan_create_2018}
\citation{li_video_2017}
\citation{saito_tganv2:_2018}
\citation{pan_create_2018}
\citation{li_video_2017}
\citation{pan_create_2018}
\citation{saito_tganv2:_2018}
\citation{pan_create_2018}
\citation{saito_tganv2:_2018}
\citation{brock_large_2018}
\citation{wang_non-local_2017}
\citation{brock_large_2018}
\citation{zhang_self-attention_2018}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Baselines}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The Language Model}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Architecture}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Why take inspiration from TGANv2?}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Differences from TGANv2}{11}\protected@file@percent }
\citation{carreira_quo_2017}
\citation{wang_non-local_2017}
\citation{pan_create_2018}
\citation{brock_large_2018}
\citation{zhang_stackgan++:_2017}
\citation{jolicoeur-martineau_relativistic_2018}
\@writefile{toc}{\contentsline {subsubsection}{Effectively Capturing Motion}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Adding Conditional Information}{11}\protected@file@percent }
\citation{pan_create_2018}
\citation{noauthor_microsoft_nodate}
\citation{pan_create_2018}
\citation{radford_language_2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experimentation}{12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Datasets}{12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1.1}{\ignorespaces Some example real videos from the dataset}}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Setup}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Evaluation}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Results, Findings and Analysis}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}MNIST moving}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{TCWYT Baseline}{13}\protected@file@percent }
\newlabel{fig:artifacts_base}{{4.4.1}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.1}{\ignorespaces Occasional artifacts from training}}{14}\protected@file@percent }
\newlabel{fig:modecollapse_base}{{4.4.1}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.2}{\ignorespaces Three generated samples with the same sentence but different latent variables (evidence of mode collapse)}}{14}\protected@file@percent }
\newlabel{fig:wrongdir_base}{{4.4.1}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.3}{\ignorespaces Video for the text `the digit 9 is moving down and up' (real is top, generated is bottom)}}{14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{My Method}{14}\protected@file@percent }
\newlabel{fig:my_no_corr}{{4.4.1}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.4}{\ignorespaces From top to bottom: `$<$start$>$ digit 9 is left and right$<$end$>$', `$<$start$>$ digit 8 is right and left$<$end$>$' `$<$start$>$ digit 8 is bottom and top$<$end$>$' and `$<$start$>$ digit 4 is top and bottom$<$end$>$'}}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}MRVDC}{15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{TCWYT Baseline}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.5}{\ignorespaces Good visual results from two different videos (sampled from the training set).}}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.6}{\ignorespaces Bad quality videos sampled from the training process.}}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.7}{\ignorespaces Three samples from the testing set with the ground truth on the bottom. For the sentence (tokenized): `$<$start$>$ a woman is saying about how to make vegetable tofu $<$unk$>$ $<$end$>$'}}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.8}{\ignorespaces Three samples from the testing set with the ground truth on the bottom. For the sentence (tokenized): `$<$start$>$ the person is cooking $<$end$>$'}}{16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{TGANv2 Baseline}{16}\protected@file@percent }
\newlabel{fig:uncond_bad}{{4.4.2}{16}}
\newlabel{fig:uncond_mr}{{4.4.2}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.9}{\ignorespaces Various generated samples from MRVDC with a re-implementation of TGANv2 (rescaled from 128x128 to fill the line). }}{16}\protected@file@percent }
\citation{noauthor_ucf101_nodate}
\citation{brock_large_2018}
\citation{saito_tganv2:_2018}
\citation{heusel_gans_2017}
\citation{heusel_gans_2017}
\citation{arjovsky_wasserstein_2017}
\newlabel{fig:uncond_mr}{{4.4.2}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.10}{\ignorespaces Some poorly generated samples from MRVDC from a training batch. }}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{My Method}{17}\protected@file@percent }
\newlabel{fig:cond_mr}{{4.4.2}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.11}{\ignorespaces Generated samples from the conditional model and their real associated video. From top to bottom: `$<$start$>$ the man poured preserves over the chicken$<$end$>$', `$<$start$>$ a person is dicing and onion$<$end$>$', `$<$start$>$ a woman is peeling a large shrimp in a glass bowl of water$<$end$>$'}}{17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Observations During Training}{17}\protected@file@percent }
\citation{ioffe_batch_2015}
\citation{pan_create_2018}
\citation{saito_tganv2:_2018}
\citation{saito_tganv2:_2018}
\newlabel{fig:wtf_samples}{{4.4.3}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4.12}{\ignorespaces When the equivalent model with the same hyper-parameters is trained on a K80 on Adelaide Uni Phoenix HPC}}{17}\protected@file@percent }
\citation{pan_create_2018}
\citation{saito_temporal_2016}
\citation{noauthor_ucf101_nodate}
\bibdata{Thesis}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{18}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Future Work}{18}\protected@file@percent }
\bibcite{noauthor_microsoft_nodate}{1}
\bibcite{noauthor_ucf101_nodate}{2}
\bibcite{arjovsky_wasserstein_2017}{3}
\bibcite{brock_large_2018}{4}
\bibcite{carreira_quo_2017}{5}
\bibcite{dai_semi-supervised_2015}{6}
\bibcite{goodfellow_deep_2016}{7}
\bibcite{goodfellow_generative_2014}{8}
\bibcite{gulrajani_improved_2017}{9}
\bibcite{he_deep_2015}{10}
\bibcite{heusel_gans_2017}{11}
\bibcite{ioffe_batch_2015}{12}
\bibcite{jacobsen_i-revnet:_2018}{13}
\bibcite{jolicoeur-martineau_relativistic_2018}{14}
\bibcite{kay_kinetics_2017}{15}
\bibcite{kingma_glow:_2018}{16}
\bibcite{li_video_2017}{17}
\bibcite{martin_miguelmartin75/txt2vid_nodate}{18}
\bibcite{mirza_conditional_2014}{19}
\bibcite{pan_create_2018}{20}
\bibcite{radford_unsupervised_2015}{21}
\bibcite{radford_language_2019}{22}
\bibcite{saito_temporal_2016}{23}
\bibcite{saito_tganv2:_2018}{24}
\bibcite{salimans_improved_2016}{25}
\bibcite{shi_convolutional_2015}{26}
\bibcite{wang_non-local_2017}{27}
\bibcite{zhang_self-attention_2018}{28}
\bibcite{zhang_stackgan++:_2017}{29}
\bibstyle{plain}
